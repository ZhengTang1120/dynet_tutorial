{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dynet_config\n",
    "dynet_config.set(\n",
    "    mem=2048,          # can probably get away with 1024\n",
    "    autobatch=True,    # utilize autobatching\n",
    "    random_seed=1978   # simply for reproducibility here\n",
    ")\n",
    "import dynet as dy\n",
    "from os import path\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `dyNet` example: `spam` v. `ham`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import utils as u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change this string to match the path on your computer\n",
    "path_to_root = \"/Users/mcapizzi/Github/dynet_tutorial/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training data; 1 == spam, 2 == ham\n",
      "loading dev data; 1 == spam, 2 == ham\n",
      "loading testing data; 1 == spam, 2 == ham\n"
     ]
    }
   ],
   "source": [
    "train_docs, train_labels, dev_docs, dev_labels, test_docs, test_labels = u.import_smsspam(path_to_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3345, 3345)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_docs), len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1115, 1115)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_docs), len(dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1114, 1114)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_docs), len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are either `1` or `0` where `1=Spam` and `0=Ham`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, the \"features\" are still just raw documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Go', 'until', 'jurong', 'point,', 'crazy..', 'Available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet...', 'Cine', 'there', 'got', 'amore', 'wat...']\n",
      "['Ok', 'lar...', 'Joking', 'wif', 'u', 'oni...']\n",
      "['Free', 'entry', 'in', '2', 'a', 'wkly', 'comp', 'to', 'win', 'FA', 'Cup', 'final', 'tkts', '21st', 'May', '2005.', 'Text', 'FA', 'to', '87121', 'to', 'receive', 'entry', 'question(std', 'txt', \"rate)T&C's\", 'apply', \"08452810075over18's\"]\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(train_docs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build architecture\n",
    "all images borrowed from here: http://u.cs.biu.ac.il/~yogo/nnlp.pdf (now a book!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![goldberg_nn](images/goldberg_ff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![goldberg_ff_math](images/goldberg_ff_math.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize empty model\n",
    "\n",
    "See http://dynet.readthedocs.io/en/latest/python_ref.html#parametercollection\n",
    "\n",
    "The first thing to be done is initialize the `ParameterCollection()` which will house all the parameters that will be updated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_dynet.ParameterCollection at 0x1129acf60>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_forward_model = dy.ParameterCollection()   # used to be called dy.Model()\n",
    "feed_forward_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to turn each document into an *average* of the embeddings for each word.  In order to do that, we have two choices:\n",
    " 1. ...randomly initialize the word embeddings.\n",
    " 2. ...load some pretrained word embeddings.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### randomly initialized embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `embedding matrix` will be of size `size_vocabulary x embedding_dim` where the $i^{th}$ row of the matrix is the embedding vector for the $i^{th}$ indexed word.\n",
    "\n",
    "So first we'll need to build the `word-2-index` lookup table of all the words in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2i_random = u.build_w2i_lookup(train_docs)\n",
    "w2i_random[\"the\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pretrained embeddings\n",
    "The embeddings we'll use are from this paper: https://levyomer.files.wordpress.com/2014/04/dependency-based-word-embeddings-acl-2014.pdf <br>\n",
    "And can be downloaded here: https://levyomer.wordpress.com/2014/04/25/dependency-based-word-embeddings/\n",
    "\n",
    "In *most* cases, these vectors are ordered by frequency, so we can safely take the `top n` words to save some computation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_matrix_pretrained, w2i_pretrained = u.load_pretrained_embeddings(\n",
    "    path.join(path_to_root, \"pretrained_embeddings.txt\"), \n",
    "    take=5000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have (1) an `embedding matrix` of size `num_words x embedding_dim` and (2) a `word-2-index` lookup table.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding matrix shape: (5001, 300)\n",
      "index for 'the': 1\n"
     ]
    }
   ],
   "source": [
    "print(\"embedding matrix shape: {}\".format(emb_matrix_pretrained.shape))\n",
    "print(\"index for '{}': {}\".format(\"the\", w2i_pretrained[\"the\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build the actual `dyNet` parameters for the embeddings.  These *will* be updated during training, which will lead to \"catered\" embeddings specialized for our `spam/ham` task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### CHOOSE HERE which approach you want to use. ######\n",
    "embedding_approach, embedding_dim = \"pretrained\", emb_matrix_pretrained.shape[1]\n",
    "# embedding_approach, embedding_dim = \"random\", 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 5001)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if embedding_approach == \"pretrained\":\n",
    "    embedding_parameters = feed_forward_model.lookup_parameters_from_numpy(emb_matrix_pretrained)\n",
    "    w2i = w2i_pretrained    # ensure we use the correct lookup table\n",
    "elif embedding_approach == \"random\":\n",
    "    embedding_parameters = feed_forward_model.add_lookup_parameters((len(w2i_random)+1, embedding_dim))\n",
    "    w2i = w2i_random        # ensure we use the correct lookup table\n",
    "else:\n",
    "    raise Exception(\"you chose poorly...\")\n",
    "dy.parameter(embedding_parameters).npvalue().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input size\n",
    "\n",
    "To generate our input, we'll be taking the average word embedding for all words in the document.  So it's size will be equal to the dimension of our word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = embedding_dim\n",
    "input_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hidden dimension\n",
    "\n",
    "You still have a decision on what size you want the `hidden` layer to be.\n",
    "\n",
    "![goldberg_math_simple](images/goldberg_ff_math_simple.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################\n",
    "# HYPERPARAMETER\n",
    "################\n",
    "# size of hidden layer\n",
    "hidden_size = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### paramater `initializer`\n",
    "See http://dynet.readthedocs.io/en/latest/python_ref.html#parameters-initializers\n",
    "\n",
    "Next we need to \"initialize\" the parameter values.  `GlorotInitializer` is a pretty standard approach *however* the `gain` parameter depends on the type of `activation` being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################\n",
    "# HYPERPARAMETER\n",
    "################\n",
    "initializer = dy.GlorotInitializer(gain=4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that the objects are `_dynet.Parameters` and *not* `expressions` until you \"wrap\" them with `dy.parameter()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(_dynet.Parameters, _dynet.Expression)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# W_1 (input x hidden) as a Parameters object\n",
    "pW_1 = feed_forward_model.add_parameters(\n",
    "    (input_size, hidden_size),\n",
    "    init=initializer\n",
    ")\n",
    "type(pW_1), type(dy.parameter(pW_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 200)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the shape of the Expression\n",
    "dy.parameter(pW_1).npvalue().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 200)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b_1 (1 x hidden) as a Parameters object\n",
    "pb_1 = feed_forward_model.add_parameters(\n",
    "    (1, hidden_size),\n",
    "    init=initializer\n",
    ")\n",
    "# check the shape\n",
    "dy.parameter(pb_1).npvalue().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# W_2 (hidden x output) as a Parameters object\n",
    "pW_2 = feed_forward_model.add_parameters(\n",
    "    (hidden_size, 1),\n",
    "    init=initializer\n",
    ")\n",
    "# check the shape\n",
    "dy.parameter(pW_2).npvalue().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b_2 (1 x output) as a Paramters object\n",
    "pb_2 = feed_forward_model.add_parameters(\n",
    "    (1, 1),\n",
    "    init=initializer\n",
    ")\n",
    "# check the shape\n",
    "dy.parameter(pb_2).npvalue().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### converting words to indexes\n",
    "\n",
    "In order to access the word embedding for a given word, we need to know its `index` from our `word2index` lookup table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def words2indexes(seq_of_words, w2i_lookup):\n",
    "    \"\"\"\n",
    "    This function converts our document into a sequence of indexes that correspond to the rows in our embedding matrix\n",
    "    :param seq_of_words: the document as a <list> of words\n",
    "    :param w2i_lookup: the lookup table of {word:index} that we built earlier\n",
    "    \"\"\"\n",
    "    seq_of_idxs = []\n",
    "    for w in seq_of_words:\n",
    "        w = w.lower()            # lowercase\n",
    "        i = w2i_lookup.get(w, 0) # we use the .get() method to allow for default return value if the word is not found\n",
    "                                 # we've reserved the 0th row of embedding matrix for out-of-vocabulary words\n",
    "        seq_of_idxs.append(i)\n",
    "    return seq_of_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[29, 330, 0]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_idxs = words2indexes([\"I\", \"like\", \"armadillos\"], w2i)\n",
    "sample_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_average_embedding(seq_of_idxs):\n",
    "    \"\"\"\n",
    "    This function will take a sequence of indexes (which represents each of the words in our document) \n",
    "    and will return an average embedding to now represent the document\n",
    "    :param seq_of_idxs: output of words2indexes()\n",
    "    \"\"\"\n",
    "    seq_of_embeddings = [embedding_parameters[i] for i in seq_of_idxs]   # embedding_parameters can be used like <dict>\n",
    "    average_embedding = dy.average(seq_of_embeddings)\n",
    "    return dy.reshape(average_embedding, (1,300))      # need to reshape to an explicit 1 x ??? vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.90775904e-03,   5.09412680e-03,   1.79931540e-02,\n",
       "         -3.60248461e-02,  -2.73635816e-02,   9.58728045e-03,\n",
       "         -2.32764576e-02,  -3.17369145e-03,   2.68593710e-02,\n",
       "         -6.22892985e-03,   2.44046245e-02,   4.14363248e-03,\n",
       "          7.23031815e-03,   3.16809834e-04,  -1.54533647e-02,\n",
       "         -2.47859117e-02,  -5.86350635e-02,  -5.79605959e-02,\n",
       "         -1.00360718e-02,   1.51551692e-02,  -3.78740542e-02,\n",
       "         -3.99237759e-02,   1.31739099e-02,  -4.71997671e-02,\n",
       "          6.86733797e-02,   5.29726222e-02,  -3.93233495e-03,\n",
       "          1.07182777e-02,   7.38620944e-03,  -2.43036561e-02,\n",
       "          1.10193091e-02,   8.02149996e-03,   6.36167591e-03,\n",
       "         -1.68125313e-02,  -2.90750396e-02,  -2.93785091e-02,\n",
       "          9.91539657e-03,   7.05230189e-03,  -1.45967472e-02,\n",
       "         -2.46045343e-03,  -1.30066145e-02,   2.21646810e-03,\n",
       "          1.33913625e-02,  -3.73893864e-02,  -8.10857303e-03,\n",
       "         -8.24909098e-03,   6.19452400e-03,   1.01481741e-02,\n",
       "         -2.80080661e-02,   2.35264320e-02,  -3.41688208e-02,\n",
       "         -3.24911736e-02,   3.13745700e-02,  -4.90015978e-03,\n",
       "          3.82968709e-02,   6.96148202e-02,   5.87880425e-02,\n",
       "         -5.57253622e-02,   2.17806012e-03,  -1.06687555e-02,\n",
       "         -1.82848480e-02,   6.04834780e-02,  -4.37608249e-02,\n",
       "         -1.99594852e-02,   4.43749204e-02,  -9.51655768e-03,\n",
       "         -2.19018031e-02,  -9.46418196e-03,   1.44491401e-02,\n",
       "          1.01515636e-01,  -6.11779187e-03,  -2.04110276e-02,\n",
       "         -4.40319255e-02,   4.90827151e-02,   6.83092400e-02,\n",
       "         -2.34445147e-02,   1.64032280e-02,  -2.21547838e-02,\n",
       "         -4.40540276e-02,  -4.97097857e-02,   3.41390520e-02,\n",
       "         -4.00401726e-02,   4.28937422e-03,   9.01962593e-02,\n",
       "         -1.35146342e-02,   1.77746136e-02,  -7.13342279e-02,\n",
       "          1.58586446e-02,   4.61300388e-02,  -9.31028463e-03,\n",
       "         -1.53002385e-02,  -7.30663631e-03,   2.14921217e-02,\n",
       "         -4.88284342e-02,   5.17409258e-02,   9.29683374e-05,\n",
       "          1.09877791e-02,  -3.34197246e-02,   4.06594612e-02,\n",
       "          4.87897499e-03,  -5.31036826e-03,  -4.96486202e-02,\n",
       "          3.14137749e-02,  -2.61447430e-02,  -3.84670384e-02,\n",
       "          1.39351247e-03,   2.04138514e-02,   3.82997096e-02,\n",
       "         -1.63889199e-03,  -2.09730100e-02,   3.89543697e-02,\n",
       "         -2.60281395e-02,   2.66721882e-02,  -2.11675130e-02,\n",
       "          3.52563187e-02,   5.87824993e-02,   1.90762095e-02,\n",
       "          3.06334533e-02,  -1.27131734e-05,   2.22691726e-02,\n",
       "         -2.20258739e-02,   7.70319030e-02,   1.26553820e-02,\n",
       "          2.45547723e-02,   6.20653061e-03,  -4.90248145e-04,\n",
       "         -3.44049074e-02,   3.51157077e-02,  -2.21067332e-02,\n",
       "          1.13432202e-02,  -3.21481787e-02,   3.42274234e-02,\n",
       "         -6.23090519e-03,   2.41607297e-02,   2.85092629e-02,\n",
       "          4.59605316e-03,   1.83126293e-02,   2.65176445e-02,\n",
       "          5.55326715e-02,  -5.34401508e-03,   2.09311359e-02,\n",
       "          2.19961051e-02,  -8.40069540e-03,   1.07106688e-02,\n",
       "          4.07135002e-02,   3.65184322e-02,  -6.50954572e-03,\n",
       "          1.54431835e-02,  -2.01786235e-02,  -1.29625043e-02,\n",
       "         -4.16993462e-02,   3.46573927e-02,   1.87441637e-03,\n",
       "          3.43130040e-03,   2.37990953e-02,   4.05881926e-02,\n",
       "          2.37626825e-02,  -4.44692858e-02,   1.87803935e-02,\n",
       "         -1.62800513e-02,  -6.74959226e-03,  -1.29106576e-02,\n",
       "         -4.31464203e-02,   2.26040017e-02,  -5.71512841e-02,\n",
       "         -6.38391599e-02,   4.75023054e-02,  -7.00024888e-03,\n",
       "         -4.44744341e-02,  -5.78781627e-02,   5.67141827e-03,\n",
       "          1.53620280e-02,  -2.20455993e-02,   1.33888749e-02,\n",
       "          1.37434225e-03,  -1.54334903e-02,   3.63672860e-02,\n",
       "         -6.18830277e-03,   2.93432400e-02,  -3.29973623e-02,\n",
       "         -4.38237237e-03,   4.13392819e-02,   3.94049473e-02,\n",
       "         -1.88259166e-02,   7.57896481e-03,   1.04343910e-02,\n",
       "         -3.02831735e-02,  -1.72880907e-02,   5.36063779e-03,\n",
       "         -8.99358932e-03,  -2.28473009e-03,   2.61364784e-02,\n",
       "          2.08498631e-02,   2.49715932e-02,  -1.10803088e-02,\n",
       "         -2.93133268e-03,  -1.08217467e-02,  -1.39115471e-02,\n",
       "          6.96156127e-03,  -2.20672432e-02,  -2.48958059e-02,\n",
       "         -5.03015285e-03,   2.20584739e-02,   3.06126326e-02,\n",
       "         -1.34700453e-02,  -4.80354652e-02,   3.32790576e-02,\n",
       "          4.47621830e-02,   3.85862924e-02,  -9.16179502e-04,\n",
       "          3.76771986e-02,  -1.38138710e-02,   6.91751912e-02,\n",
       "         -5.32510458e-03,  -1.69069301e-02,   4.41186205e-02,\n",
       "          5.51573858e-02,   1.23965899e-02,  -2.50469260e-02,\n",
       "         -2.51502804e-02,  -3.30511085e-03,   3.00031733e-02,\n",
       "         -4.85413186e-02,   8.83743633e-03,   2.60006473e-03,\n",
       "          2.31980179e-02,   5.32879345e-02,  -1.19831627e-02,\n",
       "         -2.67659053e-02,   4.99423482e-02,  -5.90601144e-03,\n",
       "         -7.98094738e-03,   9.31053050e-03,   3.02430056e-02,\n",
       "         -1.45708546e-02,   4.06350233e-02,   4.10543866e-02,\n",
       "          5.28826658e-03,   4.83789034e-02,   5.50229363e-02,\n",
       "          5.66909672e-04,   3.48752961e-02,  -6.50848597e-02,\n",
       "         -3.96603756e-02,  -1.46221686e-02,  -9.28695896e-04,\n",
       "          1.89040937e-02,  -1.55983008e-02,  -4.33103479e-02,\n",
       "         -3.33043747e-02,  -5.28359832e-03,  -1.39575070e-02,\n",
       "          1.41370799e-02,   3.50010581e-02,   1.56483296e-02,\n",
       "         -1.05361699e-03,  -1.36438105e-02,   1.81597639e-02,\n",
       "          3.27893309e-02,   7.35292770e-03,   6.09134603e-03,\n",
       "          2.64051370e-02,  -2.54044440e-02,   4.62970603e-03,\n",
       "         -2.45576557e-02,   5.48409969e-02,  -7.88947288e-03,\n",
       "         -4.61580865e-02,  -3.34200375e-02,  -7.95896649e-02,\n",
       "          2.60251621e-03,  -1.86587069e-02,   2.17001257e-03,\n",
       "         -5.06531186e-02,  -4.28558849e-02,   7.10756108e-02,\n",
       "         -1.58705972e-02,  -1.46887284e-02,  -4.60410397e-03,\n",
       "          2.51065679e-02,  -3.67219076e-02,  -4.13436778e-02,\n",
       "          2.33137340e-04,  -2.22048219e-02,   2.32957792e-03,\n",
       "         -1.77220150e-03,  -1.62478201e-02,  -1.57337524e-02,\n",
       "          1.42281025e-03,  -3.88116948e-02,   5.80941606e-03,\n",
       "          2.43134890e-02,   2.79452885e-03,  -3.94017063e-02,\n",
       "          3.20149735e-02,  -7.27569535e-02,  -1.40329944e-02,\n",
       "          4.86056097e-02,   1.21308183e-02,  -2.12807022e-03]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_average_embedding = get_average_embedding(sample_idxs)\n",
    "sample_average_embedding.npvalue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### forward operations\n",
    "![goldberg_math_simple](images/goldberg_ff_math_simple.png)\n",
    "\n",
    "The only real choice is the type of `activation`.  See here for your choices: http://dynet.readthedocs.io/en/latest/operations.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_pass(x):\n",
    "    \"\"\"\n",
    "    This function will wrap all the steps of the forward pass\n",
    "    :param x: the average word embedding (output of get_average_embedding())\n",
    "    \"\"\"\n",
    "    # convert Parameters to Expressions\n",
    "    W_1 = dy.parameter(pW_1)\n",
    "    b_1 = dy.parameter(pb_1)\n",
    "    W_2 = dy.parameter(pW_2)\n",
    "    b_2 = dy.parameter(pb_2)\n",
    "    # calculate the first hidden layer\n",
    "    hidden = x * W_1 + b_1          \n",
    "    ################\n",
    "    # HYPERPARAMETER\n",
    "    ################\n",
    "    # calculate the sigmoid activation  (or RELU, SELU, ELU, tanh, etc...)\n",
    "    hidden_activation = dy.logistic(hidden)    \n",
    "    # calculate the output layer\n",
    "    output = hidden_activation * W_2 + b_2\n",
    "    # return the sigmoid of the output\n",
    "    return dy.logistic(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initializing a `trainer`\n",
    "See http://dynet.readthedocs.io/en/latest/python_ref.html#optimizers\n",
    "\n",
    "This decision is a big one.  It relates to what \"optimizer\" will be used to update the parameters.  Here I've chosen a *very simple* `trainer`, however the default `learning_rate` is almost never a good one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################\n",
    "# HYPERPARAMETER\n",
    "################\n",
    "trainer = dy.SimpleSGDTrainer(\n",
    "    m=feed_forward_model,\n",
    "    learning_rate=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### autobatching\n",
    "See http://dynet.readthedocs.io/en/latest/minibatch.html# <br>\n",
    "and the technical details here: https://arxiv.org/pdf/1705.07860.pdf\n",
    "\n",
    "This is one of the real advantages of `dyNet`.  It's \"overkill\" for this example, but will become hugely valuable when training `recurrent neural networks` (`RNNs`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dynet_config\n",
    "dynet_config.set(\n",
    "    mem=2048,          # can probably get away with 1024\n",
    "    autobatch=True,    # utilize autobatching\n",
    "    random_seed=1978   # simply for reproducibility here\n",
    ")\n",
    "import dynet as dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### one `epoch`\n",
    "\n",
    "Let's walk through *one* epoch (where our model sees all of our data *one* time).\n",
    "\n",
    "The most important step is `dy.renew_cg()` which starts off a \"clean\" computational graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store original values of W_1\n",
    "original_W1 = dy.parameter(pW_1).npvalue()\n",
    "# begin a clean computational graph\n",
    "dy.renew_cg()\n",
    "# initialize list to capture individual losses\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`autobatching` allows us to feed each datapoint in one at a time, and `dyNet` will figure out how to \"optimize\" the operations.  Let's iterate through our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# iterate through the dataset\n",
    "for i in range(len(train_docs)):\n",
    "    # prepare input: words to indexes\n",
    "    seq_of_idxs = words2indexes(train_docs[i], w2i)\n",
    "    # prepare input: average embedding\n",
    "    x = get_average_embedding(seq_of_idxs)\n",
    "    # reshape\n",
    "    # prepare output\n",
    "    y = dy.scalarInput(train_labels[i])\n",
    "    # make a forward pass\n",
    "    pred = forward_pass(x)\n",
    "    # calculate loss for each example\n",
    "    loss = dy.binary_log_loss(pred, y) \n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's accumulate the loss and backpropogate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get total loss for dataset\n",
    "total_loss = dy.esum(losses)\n",
    "# apply the calculations of the computational graph\n",
    "total_loss.forward()\n",
    "# calculate loss to backpropogate\n",
    "total_loss.backward()\n",
    "# update parameters with backpropogated error\n",
    "trainer.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure that our parameter `W_1` has been updated (e.g. it \"learned\" something)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change in W_1 parameter values: 0.0021902552966253097\n"
     ]
    }
   ],
   "source": [
    "# confirm that parameters updated\n",
    "dy.renew_cg()\n",
    "print(\"change in W_1 parameter values: {}\".format(\n",
    "    np.sum(original_W1 - dy.parameter(pW_1).npvalue())\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make a single prediction\n",
    "\n",
    "Let's see how our model does on a single document.  The `output` can be understood as the probability the document is `Spam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9994034171104431\n"
     ]
    }
   ],
   "source": [
    "sample_pred = words2indexes(test_docs[0], w2i)\n",
    "sample_pred = get_average_embedding(sample_pred)\n",
    "pred = forward_pass(sample_pred)\n",
    "print(pred.value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get predictions on entire test set\n",
    "\n",
    "Let's look across the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "dy.renew_cg()\n",
    "for doc in test_docs:\n",
    "    idxs = words2indexes(doc, w2i)\n",
    "    ave_embedding = get_average_embedding(idxs)\n",
    "    pred = forward_pass(ave_embedding)\n",
    "    all_preds.append(pred.value())\n",
    "original_preds = all_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that the output is pretty much the same for *all* documents.  Not suprising since the model only saw each document only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9994034171104431, 0.9993950724601746, 0.9994257092475891, 0.9993324875831604, 0.9993460774421692, 0.9992807507514954, 0.9992631673812866, 0.9994104504585266, 0.9992952942848206, 0.9993500709533691, 0.9993181824684143, 0.9993802309036255, 0.999201238155365, 0.9994553327560425, 0.9992917776107788, 0.9993206262588501, 0.9993990659713745, 0.9994218349456787, 0.9993622303009033, 0.9993591904640198, 0.9992297291755676, 0.9993546009063721, 0.9993452429771423, 0.9992887377738953, 0.9993416666984558, 0.9992390275001526, 0.9991928339004517, 0.9993562698364258, 0.9994158744812012, 0.9994326829910278, 0.9994310736656189, 0.9993804097175598, 0.9993774890899658, 0.9992112517356873, 0.9992589950561523, 0.9992702007293701, 0.9994185566902161, 0.9993841052055359, 0.9992297291755676, 0.9993551969528198, 0.9993306994438171, 0.9992659687995911, 0.9991027116775513, 0.9993909001350403, 0.9993570446968079, 0.9993858933448792, 0.9993821978569031, 0.9993992447853088, 0.9992924332618713, 0.9993616938591003, 0.9992297291755676, 0.999360203742981, 0.9993771910667419, 0.9993900656700134, 0.999366044998169, 0.999379575252533, 0.9994099140167236, 0.9992713332176208, 0.999321699142456, 0.999295711517334, 0.9993577599525452, 0.999371349811554, 0.999321699142456, 0.999419629573822, 0.9992209076881409, 0.9993831515312195, 0.9993353486061096, 0.9993095397949219, 0.9994022846221924, 0.9993449449539185, 0.9993582367897034, 0.9993351697921753, 0.999370276927948, 0.9992733001708984, 0.9994516968727112, 0.9993941783905029, 0.999440610408783, 0.9993308186531067, 0.9994010329246521, 0.9993051290512085, 0.9993780851364136, 0.9993009567260742, 0.9993619322776794, 0.9992950558662415, 0.9994913339614868, 0.9993759393692017, 0.9992987513542175, 0.9993798732757568, 0.9994028210639954, 0.9994239807128906, 0.9994478225708008, 0.9993392825126648, 0.9993351697921753, 0.9993961453437805, 0.999293863773346, 0.9993827939033508, 0.9993962645530701, 0.9994204044342041, 0.9994342923164368, 0.9993777275085449]\n"
     ]
    }
   ],
   "source": [
    "print(original_preds[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_score(pred, true_y):\n",
    "    # convert pred to hard label\n",
    "    label = 1 if pred >= 0.5 else 0\n",
    "    # compare to true_y\n",
    "    return 1 if label == true_y else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_accuracy(list_of_scores):\n",
    "    return float(sum(list_of_scores) / len(list_of_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And since we predicted the same label for all documents, then our accuracy is simply matching the distribution of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13016157989228008"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = get_accuracy([check_score(p, y) for p,y in zip(all_preds, test_labels)])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multiple epochs and minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to run the model through the data many, many more times before it can learn anything meaningful.  \n",
    "\n",
    "So we need to decide (1) how many `epochs` (times through the data), and (2) how many datapoints we want to show the model at once.  If `batch_size=len(data)` then we show the model *all* the data to the model at one time (and only one parameter update is made).  If `batch_size=1` then we only show the model one item at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################\n",
    "# HYPERPARAMETER\n",
    "################\n",
    "num_epochs = 100\n",
    "################\n",
    "# HYPERPARAMETER\n",
    "################\n",
    "batch_size = 128\n",
    "################\n",
    "# HYPERPARAMETER\n",
    "################\n",
    "num_batches = int(np.ceil(len(train_docs) / batch_size))\n",
    "num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bookeeping\n",
    "original_W1 = dy.parameter(pW_1).npvalue()\n",
    "epoch_losses = []\n",
    "all_accuracies = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is code for iterating through multiple `epoch`s of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "epoch 11\n",
      "epoch 21\n",
      "epoch 31\n",
      "epoch 41\n",
      "epoch 51\n",
      "epoch 61\n",
      "epoch 71\n",
      "epoch 81\n",
      "epoch 91\n",
      "change in W_1 parameter values: -7.790721409750404\n"
     ]
    }
   ],
   "source": [
    "# iterate through epochs\n",
    "for i in range(num_epochs):\n",
    "    epoch_loss = []\n",
    "    if i % 10 == 0:\n",
    "        print(\"epoch {}\".format(i+1))\n",
    "    # shuffle dataset\n",
    "    zipped = list(zip(train_docs, train_labels))\n",
    "    random.shuffle(zipped)\n",
    "    docs, labels = zip(*zipped)\n",
    "    # iterate through batches\n",
    "    for j in range(num_batches):\n",
    "        # begin a clean computational graph *at beginning of each batch*\n",
    "        dy.renew_cg()\n",
    "        losses = []\n",
    "        # build the batch\n",
    "        batchX = train_docs[j*batch_size:(j+1)*batch_size]\n",
    "        batchY = train_labels[j*batch_size:(j+1)*batch_size]\n",
    "        # iterate through the batch\n",
    "        for k in range(len(batchX)):\n",
    "            # prepare input\n",
    "            x = words2indexes(batchX[k], w2i)\n",
    "            x = get_average_embedding(x)\n",
    "            # prepare output\n",
    "            y = dy.scalarInput(batchY[k])\n",
    "            # make a forward pass\n",
    "            pred = forward_pass(x)\n",
    "            # calculate loss for each example\n",
    "            loss = dy.binary_log_loss(pred, y)  \n",
    "            losses.append(loss)\n",
    "        # get total loss for batch\n",
    "        total_loss = dy.esum(losses)\n",
    "        # applies the calculations of the computational graph\n",
    "        total_loss.forward()\n",
    "        # calculates loss to backpropogate\n",
    "        total_loss.backward()\n",
    "        # update parameters with backpropogated error\n",
    "        trainer.update()\n",
    "        # record batch loss\n",
    "        epoch_loss.append(total_loss.npvalue())\n",
    "    # record epoch loss\n",
    "    epoch_losses.append(np.sum(epoch_loss))\n",
    "    # check performance on test set\n",
    "    all_preds = []\n",
    "    dy.renew_cg()\n",
    "    for i in range(len(test_docs)):\n",
    "        x = words2indexes(test_docs[i], w2i)\n",
    "        x = get_average_embedding(x)\n",
    "        pred = forward_pass(x)\n",
    "        all_preds.append(pred.value())\n",
    "    accuracy = get_accuracy([check_score(p, y) for p,y in zip(all_preds, list(test_labels))])\n",
    "    all_accuracies.append(accuracy)\n",
    "# confirm that parameters updated\n",
    "dy.renew_cg()\n",
    "print(\"change in W_1 parameter values: {}\".format(\n",
    "    np.sum(original_W1 - dy.parameter(pW_1).npvalue())\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x195712f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(epoch_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAH3NJREFUeJzt3Xl8VPW9//HXJwkJJIEISdgSICBBDIiAyCJugLXuC9Zb\noK1622p7W6WLtlcfWmu99Wf70Na6tVfb69LWahW5SisVkbB4rQshCBJISACFBMiEJRBCINv390cG\nGgNIIDNzJnPez8eDh3POnDnz+XrgnZPv+Z7vMeccIiLiD3FeFyAiIpGj0BcR8RGFvoiIjyj0RUR8\nRKEvIuIjCn0RER9R6IuI+IhCX0TERxT6IiI+kuB1AW1lZGS4nJwcr8sQEelUVqxYscM5l3m87aIu\n9HNycigoKPC6DBGRTsXMPm3PdureERHxEYW+iIiPKPRFRHxEoS8i4iMKfRERH1Hoi4j4iEJfRMRH\nom6cvojErkDNAf764RYampqPeK9bYgIzxw/glOREDyrzD4W+iEREfWMzN/9xBau2VGN25PvOwbPv\nbuIX153B1OF9Il+gTyj0RSQiHn6rhFVbqvndV8Zy6Rn9jnh/TcUebn95FV9/roDpY7IYlZ3mQZWf\n1adHVy4Z2Rc72k+pEFtSEiA+zjgv97gzKXSIQl9Ewm5xcYCnl23kqxMHHjXwAUZmpTHvtsk8+nYp\nTy3byNyVFRGu8uh+dtUIbjwnJ6zfsW1PHd//60dk9+zG5FMziIsL3w8Zhb6IhNXW6jpuf2UVw/t2\n557L8z5326SEeH58yXBunTqUgw1H9vtHkgN+9MoqHnhjHWcN6snIrPD85tHY1MzsF1dS39jMYzPG\nhDXwQaEvImH0j4+3cfdrazjQ0MQTs8bStUt8uz6XnJhANFzPfej6M7ns0Xe49S+F/H32eaQmhT4y\nH11UyvJPdvPIl89kSGZqyPfflkJfRA4r2V5DVc3BDu/H4XiloJx5q7YyMqsHv7p+NEN7hz/QQq1X\nSiKPzRzDjKff48dzVjFr/KCQ7r98936eWFzG9Wdlc+2Y7JDu+1gU+iLCvoONPPDGOl78cHPI9pkQ\nZ/zgomF8Z8qpdInvvLcEjR/cix9cNIxfLVzP/I+3h3z/Q3un8rOrR4R8v8ei0Bfxoa3VdVTvbzj8\n+r6/FVFRXcct5w/hC3mhGS7ZL60r2T2TQ7Ivr906dShTT+/N/vqmkO87r18PkhMjF8UKfRGfWVC0\nnW//eQXO/WtdTnoyr3xrEuNyenlXWBQzM0b0934IaSgo9EV8pHz3fn70yipG9O/BrVNygZZumHOG\npkf0bFO8o6Ms4hMNwaGBzQ6enDWWQekpXpckHlDoi0QZ5xwNTe74G56gR95eT+Hmah6bOUaB72MK\nfZGTcLCxiUcWlvJKwRZeuHkCw/v2CMl+G5qa+cbzBSxbXxWS/bU14+wBXHVm/7DsWzoHhb7ICfq4\nfA+3v/IR6yv30SXe+M3CUv77a2eFZN+PLFzPsvVV3HRODpndk0Kyz0O6d03g38YNCOk+pfNR6Iuc\ngAVF2/nuC4Wkpyby7L+fzcrN1Ty2qJTi7Xs7fLb/TmkVv1u6gRlnD+C+qyI3blv8pfPeMSESYVt2\n7eeOV1aR178Hb33/Aqac1puvT84hNSmBx/PLOrTvQM0BfvDXj8jtncpPr1TgS/joTF+kHRqampn9\n0kpw8MTMsaQldwHglOREbjxnEL9dsoHSyhpy+3TnndIqHlm4nmvHZPGVCYOOOoHWhqp93DeviM27\n9gOwt66BuoYm/nLzRLoltm9+GpGTodAXaYeH3yph5eZqnpw1loHpn73L9BvnDuHZdz/hV2+tJz01\nkRc+2Ez3pAR+8noRbxZt55fXjTp8Z2pTs+PZdzfx0IISuiXGc8GwTA79SLhmTBbD+nSPcMvEbxT6\n4iv7DjbyyML1pHXrwuxpucfd3jnHCx9s5qmlG5k1YSCXjzpyLvheKYl8bdIgnlq6ETO4+bzB3H7x\nacwtrOCBN9byxUeWkRsM8z11DWzaUctFp/fm/00/g97du4a8jSKfR6EvvvH+xp38aM4qtuyqA2Bw\nRgpXfs7wxe17DnDn3NUsKali8tB07r3i2HPBf/v8U6mubeBL47I5OziVwawJAzkvN4NfL1zPztp6\nAE5J7sKtU4YyfWxWRJ7GJNKWORf6m0A6Yty4ca6goMDrMiSG1NU38dCCEp55dxOD0pP5xfRRPLSg\nmPWV+3hj9rkMSk/BOceLH25h0brKw59b/sku6puauevS0/naxKP3zYtECzNb4Zwbd9ztFPoSywo3\n7+aOl1excUctN0waxJ2XDic5MYHy3fu57NF3yMlI4fGZY7jntTW8U7qDwRkppCS1XEjt26Mbd19+\nOoMzdPeqRL/2hr66dySq7K6t55l3NzFrwkD6pXU76jY79h3kqaUbDk8NHB9nXDMmi4lD0g9vc7Cx\nid+8XcpTSzfQL60bL3xzApOHZhx+P7tnMg9dfybf+tMKpjy8hK5d4nng2pHMGj9Q3S4S0xT6ElX+\n8uFmHs8v47l/fsJ9V444ou97/sfbuOe1NdQcaCAzteWO1X0HG3lp+RZuOieH/7xkOBuq9nH7y6so\nqazhy+MGcM8Vp9O9a5cjvuuLI/oye1ouayr2cN+VI44YlSMSixT6ElXyiwOcmplCr5REbn9lFfNW\nbWV435aRLxt31LJwbSWjstP41fUTD4+Iqatv4pdvFvPcPz/hraLtBGoOHr5jdsppvT/3+374hWFh\nb5NINGlX6JvZJcCjQDzwB+fcL9q8Pwh4BsgEdgFfdc6Vt3q/B7AWeM05d2uIapcYs6u2nsLNu5k9\nNZfZ03J59t1NPLG4jA827QQgMT6O278wjG9f+NnH73VLjOe+q0bwxRF9uff1NVx1ajo/vWLE4Ruo\nRORfjhv6ZhYPPAl8ASgHlpvZPOfc2labPQz80Tn3vJlNBR4Evtbq/f8CloWubIlFS0oCOAdTh/cm\nPs745nlD+OZ5Q9r9+UmnprPwhxeEsUKRzq89c++MB8qccxudc/XAS8DVbbbJA/KDrxe3ft/MzgL6\nAG91vFyJZfnFATJSkzgjKzYeSycSjdoT+lnAllbL5cF1ra0CpgdfXwt0N7N0M4sDfgXc0dFCJbY1\nNDWzdH0VU4dnajy8SBiFapbNO4ALzGwlcAFQATQB3wHmt+7fPxozu8XMCsysoKoqPA+PkOi24tPd\n1BxoZOrwz7/wKiId054LuRVA6ycvZAfXHeac20rwTN/MUoHrnHPVZjYJOM/MvgOkAolmts85d2eb\nzz8NPA0tN2edbGOk88ovDtAl3jg3N9PrUkRiWntCfzmQa2aDaQn7GcCs1huYWQawyznXDNxFy0ge\nnHNfabXNTcC4toEvArBoXSUTBqeTmqRRxCLhdNzuHedcI3ArsABYB7zsnCsys/vN7KrgZhcCJWa2\nnpaLtg+EqV6JQZ/urGVDVa26dkQioF2nVc65+cD8NuvubfV6DjDnOPt4DnjuhCuUmPfM/20CYNrp\nCn2RcNPjEsVTbxVt5/n3PuWmc3IYlK6JzUTCTaEvnqmoruNHc1YzMqsHd1023OtyRHxBoS+eaGxq\n5nsvrqSxqZnHZ44lKUHPhRWJBA2VEE88/96nFHy6m0dnjNZ89SIRpDN9iTjnHC99uJkxA0/h6tFt\nb+4WkXBS6EvEranYS2lgH9eNzfa6FBHfUehLxL1aWE5ifBxXjjr2Q8lFJDwU+hJR9Y3NzFu1lYvy\nemu+exEPKPQlopaur2JXbb26dkQ8otCXiJpbWE56SiLnD9PEaiJeUOhLxFTvr2fRugBXj876zOMO\nRSRy9C9PIqK52fHUso3UNzUzfayGaYp4RTdnSdhVVNfx4zmreLdsJ5ed0ZcR/Xt4XZKIbyn0JWyc\nc7xSUM79f1+Lc44Hp5/BjLMHYKbHIYp4RaEvYVG59wB3vrqaxSVVTBzSi4e+dCYDeiV7XZaI7yn0\npcOcc8wtrODjij0ANDvH6x9t5WBjEz+9Mo8bJ+XoYeciUUKhLx2ytbqO/3x1Ne+U7iA1KYFD2T6i\nfxoPXDuSIZmp3hYoIp+h0JeT9sbqbdz56mqanOPn14zkKxMGqr9eJMop9OWk7K9v5MdzVnFq71Se\nmDmWgenqrxfpDDROX07KgqLt1NY3cfdlpyvwRToRhb6clFdXVDCgVzfOzunldSkicgIU+nLCtu2p\n490NO7h2TLZG5Yh0Mgp9OWH/u7IC5+A6Tacg0uko9OWEOOd4dUU5Z+f0ZFC6nm0r0tko9OWErC7f\nw4aqWqZrPnyRTkmhLyfk1cJyEhPiuHxUP69LEZGToNCXdjv0qMOL8/rQo6sedSjSGSn0pd3yiwNU\n72/gurPUtSPSWSn0pd3mFpaT2T2J84ZmeF2KiJwkhb60y67aehaXBLhmdH8S9KhDkU5L/3qlXf62\naisNTU5dOyKdnEJf2uXVwnLy+vVgeF896lCkM1Poy3GVVtawunyPzvJFYoBCX45r7soK4uOMq87s\n73UpItJBCn35XPvrG5lbWM6FwzLJ7J7kdTki0kEKfflc975eRKDmIDefP8TrUkQkBBT6ckxzC8uZ\ns6Kc26YMZeKQdK/LEZEQUOjLUW2o2sc9r61hfE4vZk/L9bocEQmRdoW+mV1iZiVmVmZmdx7l/UFm\ntsjMVpvZEjPLDq4fbWbvmVlR8L0vh7oBEnrb9tTx3RcKSUqI49GZo3UzlkgMOe6/ZjOLB54ELgXy\ngJlmltdms4eBPzrnRgH3Aw8G1+8HbnDOjQAuAX5jZqeEqngJrUNz5V/8yDI+3bmfR748mn5p3bwu\nS0RCKKEd24wHypxzGwHM7CXgamBtq23ygB8GXy8GXgNwzq0/tIFzbquZBYBMoLrjpUsoNTQ1c9tf\nVvJm0XbOzunJQ186k5wMPSRFJNa05/f2LGBLq+Xy4LrWVgHTg6+vBbqb2Weu/JnZeCAR2ND2C8zs\nFjMrMLOCqqqq9tYuIfR/ZTt4s2g735uWy0u3TFLgi8SoUHXW3gFcYGYrgQuACqDp0Jtm1g/4E/Dv\nzrnmth92zj3tnBvnnBuXmZkZopLkROSvC9CtSzz/ceGpxOth5yIxqz3dOxXAgFbL2cF1hznnthI8\n0zezVOA651x1cLkH8AZwt3Pu/VAULaHlnCO/OMC5uRl07RLvdTkiEkbtOdNfDuSa2WAzSwRmAPNa\nb2BmGWZ2aF93Ac8E1ycC/0vLRd45oStbQml95T4qquuYOry316WISJgdN/Sdc43ArcACYB3wsnOu\nyMzuN7OrgptdCJSY2XqgD/BAcP2/AecDN5nZR8E/o0PdCOmYRcWVAEw5TaEvEuva072Dc24+ML/N\nuntbvZ4DHHEm75z7M/DnDtYoYba4OMDIrB70TevqdSkiEma668bndtfWs+LT3UzVWb6ILyj0fW5Z\naRXNDqae3sfrUkQkAhT6PrdoXYCM1ERGZaV5XYqIRIBC38cam5pZUhLgwtN6E6ex+SK+oND3sQ83\n7WLvgUamaaimiG8o9H1s7soKuiclMEWhL+IbCn2f2l/fyD8+3sblo/rpLlwRH1Ho+9Sba7ZTW9/E\n9LHZXpciIhGk0PepuYUVDOjVjbNzenpdiohEkELfh7ZW1/Huhh1MH5ONmUbtiPiJQt+HXvuoAufg\nOnXtiPiOQt9nDj0ScXxOLwamJ3tdjohEmELfZz6u2MOGqlqmj2378DMR8QOFvs+8VVRJfJxx6ch+\nXpciIh5Q6PtMfnGAswb1JC25i9eliIgHFPo+sm1PHWu37dW0CyI+ptD3kfziAIAeiyjiYwp9H8lf\nF2BAr24M7Z3qdSki4hGFvk8caGji3Q07mDa8j27IEvExhb5PvLdhJwcamjWjpojPKfR9YlFxJcmJ\n8UwY3MvrUkTEQwp9H3DOsbi4islDMzSNsojPKfR9oKSyhorqOg3VFBGFfqxrbnY8OL+YxIQ4pp6u\n0BfxO4V+jPv9OxtZur6Kn1yRR+/uXb0uR0Q8ptCPYYWbd/PQghIuHdmXr04Y6HU5IhIFFPoxak9d\nA7NfXEnftK784rpRGpsvIgAkeF2AhMfvl21ka3Udc/7jHNK6aXI1EWmhM/0Y1NzsmFtYznm5mYwd\nqGfgisi/KPRj0Psbd7J1zwGuO0uPQxSRz1Lox6BXCyvonpTAxXl9vC5FRKKMQj/G1B5s5B9rtnH5\nqH66+1ZEjqDQjzELirazv75JXTsiclQK/RjzamE5A3slM26QLuCKyJEU+jFka3Ud/9ywk+ljszQu\nX0SOSqEfQ17/aCvOwfQx6toRkaNT6MeQt9dVckZWGgPTk70uRUSiVLtC38wuMbMSMyszszuP8v4g\nM1tkZqvNbImZZbd670YzKw3+uTGUxcu/7Kqtp3Dzbj30XEQ+13FD38zigSeBS4E8YKaZ5bXZ7GHg\nj865UcD9wIPBz/YCfgpMAMYDPzUzXWEMg6XrAzgH0zR9soh8jvac6Y8HypxzG51z9cBLwNVttskD\n8oOvF7d6/4vAQufcLufcbmAhcEnHy5a2Fq0LkNk9iZH907wuRUSiWHtCPwvY0mq5PLiutVXA9ODr\na4HuZpbezs9KBzU0NbN0fRVTTsskLk6jdkTk2EJ1IfcO4AIzWwlcAFQATe39sJndYmYFZlZQVVUV\nopL8o+CT3dQcaGTqcE27ICKfrz2hXwEMaLWcHVx3mHNuq3NuunNuDHB3cF11ez4b3PZp59w459y4\nzMzME2yCLC4JkBgfx7m5GV6XIiJRrj2hvxzINbPBZpYIzADmtd7AzDLM7NC+7gKeCb5eAFxsZj2D\nF3AvDq6TDlhSEuB3SzZwoKHll6lF6yqZMKQXqUl6PIKIfL7jpoRzrtHMbqUlrOOBZ5xzRWZ2P1Dg\nnJsHXAg8aGYOWAZ8N/jZXWb2X7T84AC43zm3Kwzt8JUH5xdTUlnD3MJyvndRLhuqavnqxEFelyUi\nnYA557yu4TPGjRvnCgoKvC4japXv3s+5v1zMFaP6sfyTXVTuPQjA0h9dyKD0FI+rExGvmNkK59y4\n422n/oBOZnFxAIAffGEYGSlJ/PyNteyvb1Lgi0i7KPQ7mUXFAXLSkxmSkYKZ8dD1Z3pdkoh0Ipp7\npxPZX9/IPzfsZOrwPppFU0ROikK/E/ln2U7qG5s1v46InDSFfieyqDhASmI84wf38roUEemkFPqd\nhHOOxcUBzh+WSWKCDpuInBylRyexdttetu89wBR17YhIByj0O4n8dS1DNaecptAXkZOn0O8k8ksC\nnJmdRmb3JK9LEZFOTKHfCezYd5CPtlRrFk0R6TCFfiewpKRKT8USkZBQ6HcCi4sD9OmRxIj+Pbwu\nRUQ6OYV+lKtvbGbZ+iqmnNZbd+GKSIcp9KNcwSe7qDnYqLtwRSQkFPpRLr84QGJCHJOH6qlYItJx\nCv0ol18cYOKQdFL0VCwRCQGFfhTbtKOWjTtqmaauHREJEYV+FMsPPjBF/fkiEioK/Sj2VtF2cnun\nMqBXsteliEiMUOhHqVVbqvlg0y6uGZPldSkiEkMU+lHq8fxS0rp14YZJg7wuRURiiEI/Cq2p2MPb\n6wJ889zBdO/axetyRCSGKPSj0OP5pXTvmsCNk3O8LkVEYoxCP8qs27aXBUWVfH3yYHroLF9EQkyh\nH2WeyC8jNSmBr08e7HUpIhKDFPpRpLnZkV8c4Jox/UlL1lm+iISeQj+KVFTXUdfQxIj+aV6XIiIx\nSqEfRUoDNQDk9k71uBIRiVUK/ShSWrkPgKEKfREJE4V+FCkN7COzexKnJCd6XYqIxCiFfhQpDexT\n146IhJVCP0o45yirrFHoi0hYKfSjxLY9B6itb2Jon+5elyIiMUyhHyVKAy0XcXWmLyLhpNCPEqWV\nGq4pIuGn0I8SZYF99EpJJD01yetSRCSGKfSjhEbuiEgktCv0zewSMysxszIzu/Mo7w80s8VmttLM\nVpvZZcH1XczseTP72MzWmdldoW5ALHDOUVpZQ24fhb6IhNdxQ9/M4oEngUuBPGCmmeW12ewe4GXn\n3BhgBvDb4PrrgSTn3BnAWcC3zCwnNKXHjqqag+w90Ehub43cEZHwas+Z/nigzDm30TlXD7wEXN1m\nGwf0CL5OA7a2Wp9iZglAN6Ae2NvhqmOMRu6ISKS0J/SzgC2tlsuD61q7D/iqmZUD84HbguvnALXA\nNmAz8LBzbldHCo5Fh0buDFX3joiEWagu5M4EnnPOZQOXAX8yszhafktoAvoDg4HbzWxI2w+b2S1m\nVmBmBVVVVSEqqfMoDewjrVsXMjVyR0TCrD2hXwEMaLWcHVzX2jeAlwGcc+8BXYEMYBbwpnOuwTkX\nAN4FxrX9Aufc0865cc65cZmZmSfeik7u0MgdM/O6FBGJce0J/eVArpkNNrNEWi7UzmuzzWZgGoCZ\nnU5L6FcF108Nrk8BJgLFoSk9dpQF9mk6ZRGJiOOGvnOuEbgVWACso2WUTpGZ3W9mVwU3ux242cxW\nAS8CNznnHC2jflLNrIiWHx7POudWh6MhndXOfQfZVVtPrubcEZEISGjPRs65+bRcoG297t5Wr9cC\nk4/yuX20DNuUY9DIHRGJJN2R67HDoa+ROyISAQp9j5VV1pCalEDfHl29LkVEfECh77HS4EVcjdwR\nkUhQ6HtME62JSCQp9D1Uvb+eqpqD6s8XkYhR6Huo7PDIHQ3XFJHIUOh76NDIHd2YJSKRotD3UGnl\nPrp1iSfrlG5elyIiPqHQ91BpoIahvVOJi9PIHRGJDIW+h8o0ckdEIkyh75GaAw1s23NAc+iLSEQp\n9D2ikTsi4gWFvkc00ZqIeEGh75GywD4SE+IY0CvZ61JExEcU+h4prazh1MxU4jVyR0QiSKHvEc25\nIyJeUOh7YNueOsp31zFMI3dEJMLa9eSszqC52bGnriHs39MtMZ6uXeI7tI//XrKBhDjjmjFZIapK\nRKR9Yib0d++v56yfvx3270lNSuDuy09nxtkDTmoO/Mq9B3hx+Ra+dFY22T11EVdEIitmQj8lKYH7\nrswL+/e8tbaSu+Z+zJtrtvOL686gX9qJzZvz1NKNNDU7vnPh0DBVKCJybDET+l27xHPT5MFh/54b\nJuXw5w8+5cH5xUx6MP/w+ozURJ7/+nhG9E875mcDNQd44YNPuXZMFgPTdZYvIpEXM6EfKXFxxg2T\ncrhgWCavf7SVxmYHwF+Xb+a2v6zkb7edS0rS0f+3/n7ZRhqamvnuFJ3li4g3FPonaVB6CrOn5R5e\nPufUdGb9/n1+8toafv3l0TjneP2jrfx64XpqDzYCUF3XwNWjsxickeJV2SLicwr9EJk4JJ3Z03L5\nzdul5PXvwfJPdrGgqJIzs9M4f1gGAAlxcdxy/hCPKxURP1Poh9BtU3N5f+NOfv7GOhLj47jr0uF8\n87whuutWRKKGQj+E4uOMx2aM4bdLNjBrwkCG9dEMmiISXRT6Ida7R1fuu2qE12WIiByVpmEQEfER\nhb6IiI8o9EVEfEShLyLiIwp9EREfUeiLiPiIQl9ExEcU+iIiPmLOOa9r+AwzqwI+7cAuMoAdISqn\ns/Bjm8Gf7fZjm8Gf7T7RNg9yzmUeb6OoC/2OMrMC59w4r+uIJD+2GfzZbj+2GfzZ7nC1Wd07IiI+\notAXEfGRWAz9p70uwAN+bDP4s91+bDP4s91haXPM9emLiMixxeKZvoiIHEPMhL6ZXWJmJWZWZmZ3\nel1PuJjZADNbbGZrzazIzL4XXN/LzBaaWWnwvz29rjXUzCzezFaa2d+Dy4PN7IPgMf+rmSV6XWOo\nmdkpZjbHzIrNbJ2ZTYr1Y21mPwj+3V5jZi+aWddYPNZm9oyZBcxsTat1Rz221uKxYPtXm9nYk/3e\nmAh9M4sHngQuBfKAmWaW521VYdMI3O6cywMmAt8NtvVOYJFzLhdYFFyONd8D1rVa/iXwiHNuKLAb\n+IYnVYXXo8CbzrnhwJm0tD9mj7WZZQGzgXHOuZFAPDCD2DzWzwGXtFl3rGN7KZAb/HML8LuT/dKY\nCH1gPFDmnNvonKsHXgKu9rimsHDObXPOFQZf19ASAlm0tPf54GbPA9d4U2F4mFk2cDnwh+CyAVOB\nOcFNYrHNacD5wP8AOOfqnXPVxPixpuWJft3MLAFIBrYRg8faObcM2NVm9bGO7dXAH12L94FTzKzf\nyXxvrIR+FrCl1XJ5cF1MM7McYAzwAdDHObct+NZ2oI9HZYXLb4AfA83B5XSg2jnXGFyOxWM+GKgC\nng12a/3BzFKI4WPtnKsAHgY20xL2e4AVxP6xPuRYxzZkGRcroe87ZpYKvAp83zm3t/V7rmVIVswM\nyzKzK4CAc26F17VEWAIwFvidc24MUEubrpwYPNY9aTmrHQz0B1I4sgvEF8J1bGMl9CuAAa2Ws4Pr\nYpKZdaEl8F9wzs0Nrq489Ote8L8Br+oLg8nAVWb2CS1dd1Np6es+JdgFALF5zMuBcufcB8HlObT8\nEIjlY30RsMk5V+WcawDm0nL8Y/1YH3KsYxuyjIuV0F8O5Aav8CfScuFnnsc1hUWwL/t/gHXOuV+3\nemsecGPw9Y3A65GuLVycc3c557Kdczm0HNt859xXgMXAl4KbxVSbAZxz24EtZnZacNU0YC0xfKxp\n6daZaGbJwb/rh9oc08e6lWMd23nADcFRPBOBPa26gU6Mcy4m/gCXAeuBDcDdXtcTxnaeS8uvfKuB\nj4J/LqOlj3sRUAq8DfTyutYwtf9C4O/B10OAD4Ey4BUgyev6wtDe0UBB8Hi/BvSM9WMN/AwoBtYA\nfwKSYvFYAy/Sct2igZbf6r5xrGMLGC0jFDcAH9Myuumkvld35IqI+EisdO+IiEg7KPRFRHxEoS8i\n4iMKfRERH1Hoi4j4iEJfRMRHFPoiIj6i0BcR8ZH/DyDCXiFVyRkhAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x195202b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(all_accuracies)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "dy.renew_cg()\n",
    "for doc in test_docs:\n",
    "    idxs = words2indexes(doc, w2i)\n",
    "    ave_embedding = get_average_embedding(idxs)\n",
    "    pred = forward_pass(ave_embedding)\n",
    "    all_preds.append(pred.value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surpisingly, the model now learns how to distinguish different documents, and so the predictions range all over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6556558012962341, 0.00034403815516270697, 0.0006847633048892021, 0.00019579731451813132, 3.148395262542181e-05, 0.031864333897829056, 0.0002002134278882295, 7.159064261941239e-05, 0.0004632560012396425, 0.003368380945175886, 0.007158327382057905, 0.0035529446322470903, 0.00345473806373775, 0.9214901328086853, 0.002123923972249031, 0.007543424144387245, 0.0007220315746963024, 0.0031344410963356495, 0.9585365056991577, 2.2997510029654222e-07, 0.02617676556110382, 0.2993135452270508, 0.15958604216575623, 0.0015080824960023165, 0.0030091917142271996, 0.0005681368638761342, 0.0022377369459718466, 0.003977429121732712, 0.7797579169273376, 0.027505356818437576, 0.5282421708106995, 0.004104707855731249, 0.001457302481867373, 0.021853499114513397, 0.008700339123606682, 3.195796671207063e-05, 0.321938693523407, 0.00033812865149229765, 0.02617678977549076, 0.996843695640564, 0.5036551356315613, 0.006232960149645805, 0.0009685894474387169, 0.02478676475584507, 0.037448275834321976, 0.0002730213454924524, 0.9524284601211548, 0.0010298618581146002, 0.0011469322489574552, 0.021607190370559692, 0.02617678977549076, 0.006810491904616356, 6.242559175007045e-05, 0.00033020906266756356, 0.00026264568441547453, 0.0056915548630058765, 1.4754353287571575e-05, 0.6851378083229065, 0.010930979624390602, 0.0001253040536539629, 0.0008816872141323984, 0.00025301449932157993, 0.0014720619656145573, 0.0023682184983044863, 4.985481041330786e-07, 2.743359982559923e-05, 8.198989235097542e-05, 0.02882349118590355, 6.595665763597935e-05, 0.01897953264415264, 0.001139189349487424, 0.00042037139064632356, 0.00014400547661352903, 0.000687375373672694, 0.999561071395874, 0.0016072830185294151, 2.3949578462634236e-05, 0.003041742602363229, 0.08091346174478531, 0.004385088104754686, 0.0001458706392440945, 0.3884558081626892, 0.004574337508529425, 0.2974487543106079, 0.00834222137928009, 0.011328356340527534, 0.00014708339585922658, 0.11600127071142197, 0.0850827693939209, 0.004345520865172148, 0.002158880000934005, 0.0016637808876112103, 0.12352844327688217, 0.0049223517999053, 0.0035600215196609497, 0.009751498699188232, 0.008749638684093952, 0.5915105938911438, 6.546894155690097e-07, 0.0003805446031037718]\n"
     ]
    }
   ],
   "source": [
    "print(all_preds[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And our accuracy is fantastic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9515260323159784"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = get_accuracy([check_score(p, y) for p,y in zip(all_preds, list(test_labels))])\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 1,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
