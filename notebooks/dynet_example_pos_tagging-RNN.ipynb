{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dynet_config\n",
    "dynet_config.set(\n",
    "    mem=4096,\n",
    "    autobatch=True,      # utilize autobatching\n",
    "    random_seed=1978     # simply for reproducibility here\n",
    ")\n",
    "import dynet as dy\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `dyNet` example: `pos` tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import utils as u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change this string to match the path on your computer\n",
    "path_to_root = \"/Users/mcapizzi/Github/dynet_tutorial/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training data\n",
      "parsed 72914 sentences\n",
      "loading dev data\n",
      "parsed 7859 sentences\n",
      "loading test data\n",
      "parsed 2493 sentences\n"
     ]
    }
   ],
   "source": [
    "train_tokens, train_labels, _, _, test_tokens, test_labels = u.import_pos(path_to_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data consists of a `<list>` of `<list>` of tokens, and the labels are a `<list>` of `<list>` `pos` tags where each individual `<list>` is a training instance to be fed through the `RNN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['for', 'six', 'years', ',', 't.'], ['IN', 'CD', 'NNS', ',', 'NNP'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens[0][:5], train_labels[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to build a map from `pos tag` to an index so that we can use `int` labels in our `RNN`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('IN', 0), ('CD', 1), ('NNS', 2), (',', 3), ('NNP', 4), ('VBZ', 5), ('VBN', 6), ('JJ', 7), ('DT', 8), ('NN', 9), (':', 10), ('CC', 11), ('.', 12), ('RB', 13), ('MD', 14), ('PRP', 15), ('VB', 16), ('RBR', 17), ('VBG', 18), ('POS', 19), ('$', 20), ('PRP$', 21), ('JJR', 22), ('WDT', 23), ('VBD', 24), ('TO', 25), ('``', 26), ('VBP', 27), (\"''\", 28), ('RP', 29), ('WP', 30), ('EX', 31), ('NNPS', 32), ('JJS', 33), ('(', 34), (')', 35), ('RBS', 36), ('WRB', 37), ('FW', 38), ('UH', 39), ('WP$', 40), ('PDT', 41), ('LS', 42), ('#', 43), ('SYM', 44)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2i = u.labels_to_index_map(train_labels)\n",
    "l2i.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If you're interested, you can see https://catalog.ldc.upenn.edu/docs/LDC99T42/tagguid1.pdf for examples of each `pos tag`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now update `train_labels`, and `test_labels` to be `<list>` of `<list>` of `int`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels = [[l2i[l] for l in sent] for sent in train_labels]\n",
    "train_labels[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 4, 12]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels = [[l2i[l] for l in sent] for sent in test_labels]\n",
    "test_labels[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for use during prediction, we'll also want a reverse of that map so we can go from `int` to `pos tag`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([(0, 'IN'), (1, 'CD'), (2, 'NNS'), (3, ','), (4, 'NNP'), (5, 'VBZ'), (6, 'VBN'), (7, 'JJ'), (8, 'DT'), (9, 'NN'), (10, ':'), (11, 'CC'), (12, '.'), (13, 'RB'), (14, 'MD'), (15, 'PRP'), (16, 'VB'), (17, 'RBR'), (18, 'VBG'), (19, 'POS'), (20, '$'), (21, 'PRP$'), (22, 'JJR'), (23, 'WDT'), (24, 'VBD'), (25, 'TO'), (26, '``'), (27, 'VBP'), (28, \"''\"), (29, 'RP'), (30, 'WP'), (31, 'EX'), (32, 'NNPS'), (33, 'JJS'), (34, '('), (35, ')'), (36, 'RBS'), (37, 'WRB'), (38, 'FW'), (39, 'UH'), (40, 'WP$'), (41, 'PDT'), (42, 'LS'), (43, '#'), (44, 'SYM')])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i2l = dict((v,k) for k,v in l2i.items())\n",
    "i2l.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build architecture\n",
    "\n",
    "** all images from Chris Olah's fantastic [blog post on LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use an `RNN` to model the sequence of tokens in a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN](images/RNN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are plenty of variants out there of `RNN`s, the two most popular are `Long Short-Term Memory` (`GRU`) networks of `Gated Recurrent Unit` networks (`GRU`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_ | _\n",
    "-|-\n",
    "![LSTM](images/LSTM.png) | ![GRU](images/GRU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize empty model\n",
    "\n",
    "See http://dynet.readthedocs.io/en/latest/python_ref.html#parametercollection\n",
    "\n",
    "The first thing to be done is initialize the `ParameterCollection()` which will house all the parameters that will be updated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_dynet.ParameterCollection at 0x10b949c00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RNN_model = dy.ParameterCollection()    # used to be called dy.Model()\n",
    "RNN_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dimensions and layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have a few decisions to make on the size of your `RNN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################\n",
    "# HYPERPARAMETER\n",
    "################\n",
    "# size of word embedding (if using \"random\", otherwise, dependent on the loaded embeddings)\n",
    "embedding_size = 300\n",
    "\n",
    "################\n",
    "# HYPERPARAMETER\n",
    "################\n",
    "# size of hidden layer of `RNN`\n",
    "hidden_size = 200\n",
    "\n",
    "################\n",
    "# HYPERPARAMETER\n",
    "################\n",
    "# number of layers in `RNN`\n",
    "num_layers = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two choices for how we want to embed the tokens in our sentences:\n",
    " 1. ...randomly initialize the word embeddings.\n",
    " 2. ...load some pretrained word embeddings.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### randomly initialized embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `embedding matrix` will be of size `size_vocabulary x embedding_dim` where the $i^{th}$ row of the matrix is the embedding vector for the $i^{th}$ indexed word.\n",
    "\n",
    "So first we'll need to build the `word-2-index` lookup table of all the words in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2i_random = u.build_w2i_lookup(train_tokens)\n",
    "w2i_random[\"the\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pretrained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embeddings we'll use are from this paper: https://levyomer.files.wordpress.com/2014/04/dependency-based-word-embeddings-acl-2014.pdf <br>\n",
    "And can be downloaded here: https://levyomer.wordpress.com/2014/04/25/dependency-based-word-embeddings/\n",
    "\n",
    "In *most* cases, these vectors are ordered by frequency, so we can safely take the `top n` words to save some computation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# emb_matrix_pretrained, w2i_pretrained = u.load_pretrained_embeddings(\n",
    "#     path.join(path_to_root, \"pretrained_embeddings.txt\"), \n",
    "#     take=10000\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have (1) an `embedding matrix` of size `num_words x embedding_dim` and (2) a `word-2-index` lookup table.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(\"embedding matrix shape: {}\".format(emb_matrix_pretrained.shape))\n",
    "# print(\"index for '{}': {}\".format(\"the\", w2i_pretrained[\"the\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build the actual `dyNet` parameters for the embeddings.  These *will* be updated during training, which will lead to \"catered\" embeddings specialized for our `spam/ham` task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### CHOOSE HERE which approach you want to use. ######\n",
    "# embedding_approach, embedding_dim = \"pretrained\", emb_matrix_pretrained.shape[1]\n",
    "embedding_approach, embedding_dim = \"random\", embedding_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 37890)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if embedding_approach == \"pretrained\":\n",
    "    embedding_parameters = RNN_model.lookup_parameters_from_numpy(emb_matrix_pretrained)\n",
    "    w2i = w2i_pretrained    # ensure we use the correct lookup table\n",
    "elif embedding_approach == \"random\":\n",
    "    embedding_parameters = RNN_model.add_lookup_parameters((len(w2i_random)+1, embedding_dim))\n",
    "    w2i = w2i_random        # ensure we use the correct lookup table\n",
    "else:\n",
    "    raise Exception(\"you chose poorly...\")\n",
    "dy.parameter(embedding_parameters).npvalue().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add `RNN` unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dyNet` offers a few variants of the `RNN`, each of which is packaged in a simple \"bundle\" for you that can be built in one line.\n",
    "\n",
    "They all take *four* arguments:\n",
    " - number of layers in RNN\n",
    " - size of input (embeddings)\n",
    " - size of hidden layer\n",
    " - a `ParameterCollection()` to add the unit to\n",
    " \n",
    "See http://dynet.readthedocs.io/en/latest/builders.html for all of your options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_dynet.GRUBuilder at 0x111522f10>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### CHOOSE HERE which approach you want to use. ######\n",
    "# RNN_unit = dy.LSTMBuilder(num_layers, embedding_dim, hidden_size, RNN_model)\n",
    "RNN_unit = dy.GRUBuilder(num_layers, embedding_dim, hidden_size, RNN_model)\n",
    "RNN_unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add projection layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unless we set the `hidden_dim` of your `RNN` to be exactly the same size as the number of `pos_tags` (which is probably too small), we'll need to project the output of our `RNN`'s `hidden layer` down so that it is equal to the number of `pos tags`.\n",
    "\n",
    "So we'll add a `parameter matrix` and a `bias`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 45)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# W (hidden x num_labels) \n",
    "pW = RNN_model.add_parameters(\n",
    "        (hidden_size, len(list(l2i.keys())))\n",
    ")\n",
    "dy.parameter(pW).npvalue().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b (1 x num_labels)\n",
    "pb = RNN_model.add_parameters(\n",
    "        (len(list(l2i.keys())))        \n",
    ")\n",
    "# note: we are just giving one dimension (ignoring the \"1\" dimension)\n",
    "# this makes manipulating the shapes in forward_pass() below easier \n",
    "dy.parameter(pb).npvalue().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### converting words to indexes\n",
    "\n",
    "In order to access the word embedding for a given word, we need to know its `index` from our `word2index` lookup table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def words2indexes(seq_of_words, w2i_lookup):\n",
    "    \"\"\"\n",
    "    This function converts our sentence into a sequence of indexes that correspond to the rows in our embedding matrix\n",
    "    :param seq_of_words: the document as a <list> of words\n",
    "    :param w2i_lookup: the lookup table of {word:index} that we built earlier\n",
    "    \"\"\"\n",
    "    seq_of_idxs = []\n",
    "    for w in seq_of_words:\n",
    "        w = w.lower()            # lowercase\n",
    "        i = w2i_lookup.get(w, 0) # we use the .get() method to allow for default return value if the word is not found\n",
    "                                 # we've reserved the 0th row of embedding matrix for out-of-vocabulary words\n",
    "        seq_of_idxs.append(i)\n",
    "    return seq_of_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[146, 30, 29006]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_idxs = words2indexes([\"I\", \"like\", \"armadillos\"], w2i)\n",
    "sample_idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## forward operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_pass(x):\n",
    "    \"\"\"\n",
    "    This function will wrap all the steps needed to feed one sentence through the RNN\n",
    "    :param x: a <list> of indices\n",
    "    \"\"\"\n",
    "    # convert sequence of ints to sequence of embeddings\n",
    "    input_seq = [embedding_parameters[i] for i in x]   # embedding_parameters can be used like <dict>\n",
    "    # convert Parameters to Expressions\n",
    "    W = dy.parameter(pW)\n",
    "    b = dy.parameter(pb)\n",
    "    # initialize the RNN unit\n",
    "    rnn_seq = RNN_unit.initial_state()\n",
    "    # run each timestep through the RNN\n",
    "    rnn_hidden_outs = rnn_seq.transduce(input_seq)\n",
    "    # project each timestep's hidden output to size of labels\n",
    "    rnn_outputs = [dy.transpose(W) * h + b for h in rnn_hidden_outs]\n",
    "    return rnn_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of `forward_pass` is a `<list>` of `vector`s (one for each timestep) of size `num_labels`, and the values of each vector represent the weight the network gives to each label.  The `max` value represents the predicted `pos tag` for that token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.14690343, -0.17027847, -0.10914128, -0.30618301, -0.1283239 ,\n",
       "       -0.02966462, -0.08975635,  0.15801041,  0.01355127,  0.11134554,\n",
       "        0.01307148,  0.24266118,  0.09195352, -0.06577423, -0.05001297,\n",
       "        0.190285  ,  0.16310844, -0.07441358,  0.1705557 ,  0.24324197,\n",
       "        0.22315116,  0.07148514,  0.18567711, -0.20747925, -0.044515  ,\n",
       "        0.21493933,  0.24246149,  0.32282019,  0.05646953, -0.09182185,\n",
       "       -0.09205592, -0.11433235, -0.22574994, -0.00709864,  0.20758262,\n",
       "       -0.04263326,  0.03176683,  0.14376125, -0.05891509,  0.19858375,\n",
       "       -0.17108503, -0.24123117,  0.05611552, -0.08769525,  0.14830303])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentence = \"i own 7 armadillos .\".split()\n",
    "sample = forward_pass(words2indexes(sample_sentence, w2i))\n",
    "sample[0].npvalue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### making predictions\n",
    "\n",
    "In order to make predictions, we need to take the `argmax` value of the output for **each** token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(list_of_outputs):\n",
    "    \"\"\"\n",
    "    This function will convert the outputs from forward_pass() to a <list> of label indexes\n",
    "    \"\"\"\n",
    "    # take the softmax of each timestep\n",
    "    # note: this step isn't actually necessary as the argmax of the raw outputs will come out the same\n",
    "    # but the softmax is more \"interpretable\" if needed for debugging\n",
    "    pred_probs = [dy.softmax(o) for o in list_of_outputs]     \n",
    "    # convert each timestep's output to a numpy array\n",
    "    pred_probs_np = [o.npvalue() for o in pred_probs]\n",
    "    # take the argmax for each step\n",
    "    pred_probs_idx = [np.argmax(o) for o in pred_probs_np]\n",
    "    return pred_probs_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27, 19, 19, 15, 11]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_predict = predict(sample)\n",
    "sample_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then easily revert these easily to their `pos tags`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 'VBP'), ('own', 'POS'), ('7', 'POS'), ('armadillos', 'PRP'), ('.', 'CC')]\n"
     ]
    }
   ],
   "source": [
    "sample_predict_labels = [i2l[p] for p in sample_predict]\n",
    "print(list(zip(sample_sentence, sample_predict_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initializing a `trainer`\n",
    "See http://dynet.readthedocs.io/en/latest/python_ref.html#optimizers\n",
    "\n",
    "This decision is a big one.  It relates to what \"optimizer\" will be used to update the parameters.  Here I've chosen a *very simple* `trainer`, however the default `learning_rate` is almost never a good one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################\n",
    "# HYPERPARAMETER\n",
    "################\n",
    "trainer = dy.SimpleSGDTrainer(\n",
    "    m=RNN_model,\n",
    "    learning_rate=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### autobatching\n",
    "See http://dynet.readthedocs.io/en/latest/minibatch.html#automatic-mini-batching <br>\n",
    "and the technical details here: https://arxiv.org/pdf/1705.07860.pdf\n",
    "\n",
    "This is one of the real advantages of `dyNet` that is incredibly valuable when training `RNN`s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### one `epoch`\n",
    "\n",
    "Let's walk through *one* epoch (where our model sees all of our data *one* time).\n",
    "\n",
    "The most important step is `dy.renew_cg()` which starts off a \"clean\" computational graph.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`autobatching` allows us to feed each datapoint in one at a time, and `dyNet` will figure out how to \"optimize\" the operations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the training data is large (as is the case for us), we can also feed in smaller batches of data for `autobatch`ing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(285, 10)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################\n",
    "# HYPERPARAMETER\n",
    "################\n",
    "batch_size = 256\n",
    "num_batches_training = int(np.ceil(len(train_tokens) / batch_size))\n",
    "num_batches_testing = int(np.ceil(len(test_tokens) / batch_size))\n",
    "num_batches_training, num_batches_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1\n",
      "[('i', 'VBP'), ('own', 'POS'), ('7', 'POS'), ('armadillos', 'PRP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# iterate through the first 3 batches of training data (~1500 sentences)\n",
    "\n",
    "# j = batch index\n",
    "# k = sentence index (inside batch j)\n",
    "# l = token index (inside sentence k)\n",
    "# Note: we are reserving `i` as an index over epochs\n",
    "\n",
    "for j in range(3):\n",
    "    # begin a clean computational graph\n",
    "    dy.renew_cg()\n",
    "    # build the batch\n",
    "    batch_tokens = train_tokens[j*batch_size:(j+1)*batch_size]\n",
    "    batch_labels = train_labels[j*batch_size:(j+1)*batch_size]\n",
    "    # iterate through the batch\n",
    "    for k in range(len(batch_tokens)):\n",
    "        # prepare input: words to indexes\n",
    "        seq_of_idxs = words2indexes(batch_tokens[k], w2i)\n",
    "        # make a forward pass\n",
    "        preds = forward_pass(seq_of_idxs)\n",
    "        # calculate loss for each token in each example\n",
    "        loss = [dy.pickneglogsoftmax(preds[l], batch_labels[k][l]) for l in range(len(preds))]\n",
    "        # sum the loss for each token\n",
    "        sent_loss = dy.esum(loss)\n",
    "        # backpropogate the loss for the sentence\n",
    "        sent_loss.backward()\n",
    "        trainer.update()\n",
    "    if j % 5 == 0:\n",
    "        print(\"batch {}\".format(j+1))\n",
    "        sample = forward_pass(words2indexes(sample_sentence, w2i))\n",
    "        predictions = [i2l[p] for p in predict(sample)]\n",
    "        print(list(zip(sample_sentence, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_score(pred, true_y):\n",
    "    return 1 if pred == true_y else 0\n",
    "\n",
    "def check_sentence_score(sentence_scores):\n",
    "    return 0 if 0 in sentence_scores else 1\n",
    "\n",
    "def get_accuracy(flat_list_of_scores):\n",
    "    return float(sum(flat_list_of_scores) / len(flat_list_of_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(nested_preds, nested_true):\n",
    "    flat_scores = []\n",
    "    sentence_scores = []\n",
    "    for i in range(len(nested_true)):\n",
    "        scores = []\n",
    "        pred = nested_preds[i]\n",
    "        true = nested_true[i]\n",
    "        for p,t in zip(pred,true):\n",
    "            score = check_score(p,t)\n",
    "            scores.append(score)\n",
    "        sentence_scores.append(check_sentence_score(scores))\n",
    "        flat_scores.extend(scores)\n",
    "    overall_accuracy = get_accuracy(flat_scores)\n",
    "    sentence_accuracy = get_accuracy(sentence_scores)\n",
    "    return overall_accuracy, sentence_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same `autobatch`ing format for getting predictions for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    # j = batch index\n",
    "    # k = sentence index (inside batch j)\n",
    "    # l = token index (inside sentence k)\n",
    "    all_predictions = []\n",
    "\n",
    "    for j in range(num_batches_testing):\n",
    "        # begin a clean computational graph\n",
    "        dy.renew_cg()\n",
    "        # build the batch\n",
    "        batch_tokens = test_tokens[j*batch_size:(j+1)*batch_size]\n",
    "        batch_labels = test_tokens[j*batch_size:(j+1)*batch_size]\n",
    "        # iterate through the batch\n",
    "        for k in range(len(batch_tokens)):\n",
    "            # prepare input: words to indexes\n",
    "            seq_of_idxs = words2indexes(batch_tokens[k], w2i)\n",
    "            # make a forward pass\n",
    "            preds = forward_pass(seq_of_idxs)\n",
    "            label_preds = predict(preds)\n",
    "            all_predictions.append(label_preds)\n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_predictions = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall accuracy: 0.10721283166738618\n",
      "sentence accuracy (all tags in sentence correct): 0.1183313277176093\n"
     ]
    }
   ],
   "source": [
    "overall_accuracy, sentence_accuracy = evaluate(final_predictions, test_labels)\n",
    "print(\"overall accuracy: {}\".format(overall_accuracy))\n",
    "print(\"sentence accuracy (all tags in sentence correct): {}\".format(sentence_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multiple epochs\n",
    "\n",
    "Obviously we need to run the model through the data multiple times to see the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################\n",
    "# HYPERPARAMETER\n",
    "################\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "\n",
    "    # i = epoch index\n",
    "    # j = batch index\n",
    "    # k = sentence index (inside batch j)\n",
    "    # l = token index (inside sentence k)\n",
    "\n",
    "    epoch_losses = []\n",
    "    overall_accuracies = []\n",
    "    sentence_accuracies = []\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        epoch_loss = []\n",
    "        for j in range(num_batches_training):\n",
    "            # begin a clean computational graph\n",
    "            dy.renew_cg()\n",
    "            # build the batch\n",
    "            batch_tokens = train_tokens[j*batch_size:(j+1)*batch_size]\n",
    "            batch_labels = train_labels[j*batch_size:(j+1)*batch_size]\n",
    "            # iterate through the batch\n",
    "            for k in range(len(batch_tokens)):\n",
    "                # prepare input: words to indexes\n",
    "                seq_of_idxs = words2indexes(batch_tokens[k], w2i)\n",
    "                # make a forward pass\n",
    "                preds = forward_pass(seq_of_idxs)\n",
    "                # calculate loss for each token in each example\n",
    "                loss = [dy.pickneglogsoftmax(preds[l], batch_labels[k][l]) for l in range(len(preds))]\n",
    "                # sum the loss for each token\n",
    "                sent_loss = dy.esum(loss)\n",
    "                # backpropogate the loss for the sentence\n",
    "                sent_loss.backward()\n",
    "                trainer.update()\n",
    "                epoch_loss.append(sent_loss.npvalue())\n",
    "            # check prediction of sample sentence\n",
    "            if j % 250 == 0:\n",
    "                print(\"epoch {}, batch {}\".format(i+1, j+1))\n",
    "                sample = forward_pass(words2indexes(sample_sentence, w2i))\n",
    "                predictions = [i2l[p] for p in predict(sample)]\n",
    "                print(list(zip(sample_sentence, predictions)))\n",
    "        # record epoch loss\n",
    "        epoch_losses.append(np.sum(epoch_loss))\n",
    "        # get accuracy on test set\n",
    "        print(\"testing after epoch {}\".format(i+1))\n",
    "        epoch_predictions = test()\n",
    "        epoch_overall_accuracy, epoch_sentence_accuracy = evaluate(epoch_predictions, test_labels)\n",
    "        overall_accuracies.append(epoch_overall_accuracy)\n",
    "        sentence_accuracies.append(epoch_sentence_accuracy)\n",
    "        \n",
    "    return epoch_losses, overall_accuracies, sentence_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, batch 1\n",
      "[('i', 'IN'), ('own', 'NN'), ('7', 'DT'), ('armadillos', 'DT'), ('.', 'NNS')]\n",
      "epoch 1, batch 251\n",
      "[('i', 'PRP'), ('own', 'VBP'), ('7', 'CD'), ('armadillos', 'NNS'), ('.', '.')]\n",
      "testing after epoch 1\n",
      "epoch 2, batch 1\n",
      "[('i', 'PRP'), ('own', 'VBP'), ('7', 'CD'), ('armadillos', 'NNS'), ('.', '.')]\n",
      "epoch 2, batch 251\n",
      "[('i', 'PRP'), ('own', 'VBP'), ('7', 'CD'), ('armadillos', 'NNS'), ('.', '.')]\n",
      "testing after epoch 2\n",
      "epoch 3, batch 1\n",
      "[('i', 'PRP'), ('own', 'VBP'), ('7', 'CD'), ('armadillos', 'NNS'), ('.', '.')]\n",
      "epoch 3, batch 251\n",
      "[('i', 'PRP'), ('own', 'VBP'), ('7', 'CD'), ('armadillos', 'NNS'), ('.', '.')]\n",
      "testing after epoch 3\n",
      "epoch 4, batch 1\n",
      "[('i', 'PRP'), ('own', 'VBP'), ('7', 'CD'), ('armadillos', 'NNS'), ('.', '.')]\n",
      "epoch 4, batch 251\n",
      "[('i', 'PRP'), ('own', 'VBP'), ('7', 'CD'), ('armadillos', 'NNS'), ('.', '.')]\n",
      "testing after epoch 4\n",
      "epoch 5, batch 1\n",
      "[('i', 'PRP'), ('own', 'VBP'), ('7', 'CD'), ('armadillos', 'NNS'), ('.', '.')]\n",
      "epoch 5, batch 251\n",
      "[('i', 'PRP'), ('own', 'VBP'), ('7', 'CD'), ('armadillos', 'NNS'), ('.', '.')]\n",
      "testing after epoch 5\n"
     ]
    }
   ],
   "source": [
    "losses, overall_accs, sentence_accs = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAD8CAYAAACyyUlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22058d358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### overall accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhAPBzMwA+P++QqXLZRhinwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2207e67b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(overall_accs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sentence accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22080f7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(sentence_accs)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
